#!/usr/bin/env python3
"""
Test Runner
Runs all tests and provides a comprehensive report
"""

import asyncio
import sys
import os
import time
from typing import List, Dict, Any

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# Import test modules
from test_concurrent_downloads import test_concurrent_downloads
from test_bot_integration import test_bot_integration
from test_performance_metrics import test_performance_metrics
from test_error_handling import test_error_handling

async def run_all_tests():
    """Run all tests and provide comprehensive report"""
    print("ğŸ§ª Comprehensive Test Suite")
    print("=" * 60)
    print(f"â° Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Test configuration
    tests = [
        {
            'name': 'Concurrent Downloads',
            'description': 'Tests true concurrent file downloads without blocking',
            'function': test_concurrent_downloads,
            'critical': True
        },
        {
            'name': 'Bot Integration',
            'description': 'Tests bot integration with concurrent file handling',
            'function': test_bot_integration,
            'critical': True
        },
        {
            'name': 'Performance Metrics',
            'description': 'Tests performance tracking and logging features',
            'function': test_performance_metrics,
            'critical': False
        },
        {
            'name': 'Error Handling',
            'description': 'Tests error handling and edge cases',
            'function': test_error_handling,
            'critical': False
        }
    ]
    
    # Run tests
    test_results = []
    overall_start_time = time.time()
    
    for i, test in enumerate(tests, 1):
        print(f"ğŸ”¬ Test {i}/{len(tests)}: {test['name']}")
        print(f"   ğŸ“ {test['description']}")
        print(f"   {'ğŸ”´ Critical' if test['critical'] else 'ğŸŸ¡ Optional'}")
        print()
        
        test_start_time = time.time()
        
        try:
            result = await test['function']()
            test_duration = time.time() - test_start_time
            
            test_results.append({
                'name': test['name'],
                'description': test['description'],
                'critical': test['critical'],
                'passed': result,
                'duration': test_duration,
                'error': None
            })
            
            status = "âœ… PASSED" if result else "âŒ FAILED"
            print(f"   {status} in {test_duration:.2f}s")
            
        except Exception as e:
            test_duration = time.time() - test_start_time
            
            test_results.append({
                'name': test['name'],
                'description': test['description'],
                'critical': test['critical'],
                'passed': False,
                'duration': test_duration,
                'error': str(e)
            })
            
            print(f"   ğŸ’¥ ERROR in {test_duration:.2f}s: {e}")
        
        print()
    
    total_duration = time.time() - overall_start_time
    
    # Generate comprehensive report
    print("ğŸ“Š Test Results Summary")
    print("=" * 60)
    print(f"â° Completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â±ï¸ Total Duration: {total_duration:.2f}s")
    print()
    
    # Test results table
    print("ğŸ“‹ Test Results:")
    print("-" * 80)
    print(f"{'Test Name':<20} {'Status':<8} {'Duration':<10} {'Critical':<8} {'Notes'}")
    print("-" * 80)
    
    for result in test_results:
        status = "âœ… PASS" if result['passed'] else "âŒ FAIL"
        critical = "ğŸ”´ YES" if result['critical'] else "ğŸŸ¡ NO"
        duration = f"{result['duration']:.2f}s"
        notes = "ERROR" if result['error'] else ""
        
        print(f"{result['name']:<20} {status:<8} {duration:<10} {critical:<8} {notes}")
    
    print("-" * 80)
    
    # Statistics
    total_tests = len(test_results)
    passed_tests = len([r for r in test_results if r['passed']])
    failed_tests = total_tests - passed_tests
    critical_tests = len([r for r in test_results if r['critical']])
    critical_passed = len([r for r in test_results if r['critical'] and r['passed']])
    critical_failed = critical_tests - critical_passed
    
    print(f"\nğŸ“ˆ Statistics:")
    print(f"   ğŸ“Š Total Tests: {total_tests}")
    print(f"   âœ… Passed: {passed_tests}")
    print(f"   âŒ Failed: {failed_tests}")
    print(f"   ğŸ“Š Success Rate: {passed_tests/total_tests:.1%}")
    print()
    print(f"   ğŸ”´ Critical Tests: {critical_tests}")
    print(f"   âœ… Critical Passed: {critical_passed}")
    print(f"   âŒ Critical Failed: {critical_failed}")
    print(f"   ğŸ“Š Critical Success Rate: {critical_passed/critical_tests:.1%}" if critical_tests > 0 else "   ğŸ“Š Critical Success Rate: N/A")
    
    # Performance analysis
    avg_duration = sum(r['duration'] for r in test_results) / total_tests
    fastest_test = min(test_results, key=lambda x: x['duration'])
    slowest_test = max(test_results, key=lambda x: x['duration'])
    
    print(f"\nâš¡ Performance Analysis:")
    print(f"   â±ï¸ Average Test Duration: {avg_duration:.2f}s")
    print(f"   ğŸš€ Fastest Test: {fastest_test['name']} ({fastest_test['duration']:.2f}s)")
    print(f"   ğŸŒ Slowest Test: {slowest_test['name']} ({slowest_test['duration']:.2f}s)")
    
    # Failed tests details
    failed_tests_list = [r for r in test_results if not r['passed']]
    if failed_tests_list:
        print(f"\nâŒ Failed Tests Details:")
        for result in failed_tests_list:
            print(f"   ğŸ”´ {result['name']}: {result['error'] or 'Test failed'}")
    
    # Overall result
    all_critical_passed = critical_failed == 0
    overall_success_rate = passed_tests / total_tests
    
    print(f"\nğŸ¯ Overall Result:")
    print(f"   ğŸ“Š Overall Success Rate: {overall_success_rate:.1%}")
    print(f"   ğŸ”´ Critical Tests: {'âœ… ALL PASSED' if all_critical_passed else 'âŒ SOME FAILED'}")
    
    if all_critical_passed and overall_success_rate >= 0.8:
        print(f"   ğŸ‰ OVERALL: âœ… PASSED - System is ready for production!")
        overall_passed = True
    elif all_critical_passed:
        print(f"   âš ï¸ OVERALL: âš ï¸ PARTIAL - Critical tests passed but some optional tests failed")
        overall_passed = True
    else:
        print(f"   ğŸ’¥ OVERALL: âŒ FAILED - Critical tests failed, system needs fixes")
        overall_passed = False
    
    # Recommendations
    print(f"\nğŸ’¡ Recommendations:")
    if all_critical_passed and overall_success_rate >= 0.9:
        print(f"   âœ… Excellent! All systems are working optimally.")
        print(f"   ğŸš€ Ready for production deployment.")
    elif all_critical_passed:
        print(f"   âœ… Good! Critical functionality is working.")
        print(f"   ğŸ”§ Consider fixing failed optional tests for better performance.")
    else:
        print(f"   ğŸ”§ Critical issues found that need immediate attention.")
        print(f"   ğŸ› ï¸ Fix failed critical tests before deployment.")
    
    if failed_tests_list:
        print(f"   ğŸ“ Focus on these failed tests:")
        for result in failed_tests_list:
            if result['critical']:
                print(f"      ğŸ”´ {result['name']} (CRITICAL)")
            else:
                print(f"      ğŸŸ¡ {result['name']} (Optional)")
    
    print(f"\nğŸ Test suite completed!")
    
    return overall_passed

if __name__ == '__main__':
    result = asyncio.run(run_all_tests())
    exit(0 if result else 1)
